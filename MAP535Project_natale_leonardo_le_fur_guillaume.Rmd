---
title: "MAP535 Project"
author: "Leonardo Natale - Guillaume Le Fur"
date: "08/12/2019"
output: pdf_document
---

```{r Setup chunk, include=FALSE}
library(plotly)
library(leaps)
library(MASS)
library(caret)
library(dplyr)
<<<<<<< HEAD
library(car)
=======
library(lmtest)
>>>>>>> 6c6d24ce1e67e588a268b4450e72bd6defd364f7
knitr::opts_chunk$set(echo = FALSE)
load("data/DataProject.RData")

row.names(train) <- train$Id
train <- subset(train,select=c(-Id))
row.names(test) <- test$Id
test <- subset(test,select=c(-Id))

y.colname <- "SalePrice"
train.y <- train %>% select(SalePrice) %>% "$"(SalePrice)
train.x <- train %>% select(-SalePrice)

test.x <- test %>% select(-SalePrice)
test.y <- test %>% select(SalePrice) %>% "$"(SalePrice)

full <- train %>% 
  mutate(type = "train") %>% # mutate creates a new variable
  rbind(test %>% mutate(type = "test")) %>% 
  transform(type = as.factor(type))


full.modeled <- model.matrix(SalePrice ~ ., data = full) # creates dummy variables
train.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 1) %>% 
  dplyr::select(-typetrain) %>% 
  as.matrix()
test.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 0) %>%
  select(-typetrain) %>% 
  as.matrix()
```
Many Columns with low counts.
```{r}
summary(full)
```
# Attempt to reduce categories by grouping

```{r}
library(forcats)
full <- subset(full,select=c(-Utilities)) # only 1 observation does not have everything
full <- subset(full,select=c(-Street)) # Almost all values are Pave
full$LotConfig <- fct_collapse(full$LotConfig, FR23 = c("FR2", "FR3")) # join 2 cat
full$LandSlope <- fct_collapse(full$LandSlope, Mod=c("Mod", "Sev"))
full$Condition1 <- fct_collapse(full$Condition1, RRN=c("RRNn", "RRAn"), RRE=c("RRAe", "RRNe")) # join proximity to rail roads
full <- subset(full,select=c(-Condition2)) # Almost all values are Norm
full$RoofStyle <- fct_collapse(full$RoofStyle, Gable=c("Gable", "Shed")) # On Wikipedia are sinonyms
full <- subset(full,select=c(-RoofMatl)) # Most Standard, hard to group
full$Exterior1st <- fct_collapse(full$Exterior1st, Shng=c("AsbShng", "AsphShn"), Brk=c("BrkComm", "BrkFace"), Stucco=c("ImStucc", "Stucco"), Other=c("CBlock", "Stone"))
full <- subset(full,select=c(-Exterior2nd)) # Value always equal to Exterior1st
full$ExterCond <- fct_collapse(full$ExterCond, BlA=c("Fa", "Po"), AbA=c("Gd", "Ex")) # BlA = Below Average, AbA= Above Average
full$BsmtCond <- fct_collapse(full$BsmtCond, BlA=c("Fa", "Po"), AbA=c("Gd")) # BlA = Below Average, AbA= Above Average
full$Heating <- fct_collapse(full$Heating, FWF=c("Floor", "Wall")) # Merge wall and floor furnace
full$HeatingQC <- fct_collapse(full$HeatingQC, BlA=c("Fa", "Po"), AbA=c("Gd", "Ex")) # BlA = Below Average, AbA= Above Average
full$Electrical <- fct_collapse(full$Electrical, Standard="SBrkr", Other=c("FuseA", "FuseF", "FuseP", "Mix")) # Not sure about this one
# What about column Functional?
full$GarageQual <- fct_collapse(full$GarageQual, BlA=c("Fa", "Po"), AbA=c("Gd", "Ex")) # BlA = Below Average, AbA= Above Average
full$GarageCond <- fct_collapse(full$GarageCond, BlA=c("Fa", "Po"), AbA=c("Gd", "Ex")) # BlA = Below Average, AbA= Above Average
# What about column SaleType and SaleCondition?
cat(dim(full)) # 5 columns have been removed
```
Recreate modeled data
```{r}
full.modeled <- model.matrix(SalePrice ~ ., data = full) # creates dummy variables

train.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 1) %>% 
  dplyr::select(-typetrain) %>% 
  as.matrix()

test.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 0) %>%
  select(-typetrain) %>% 
  as.matrix()

train <- full %>%
  as.data.frame() %>% 
  filter(type == "train") %>% 
  dplyr::select(-type)

test <- full %>%
  as.data.frame() %>% 
  filter(type == "test") %>% 
  dplyr::select(-type)
```


# Introduction

## Purpose of this document

The aim of this document is to summarise and apply what has been covered during the MAP535 course on a real case study. The case study we are interested in is the prediction of the price of a house in the city of Ames, Iowa, given a certain number of predictors.
Even though the aim of such projects is to give the best possible model for prediction, we chose not to focus on this part because this is a part of data science which is already covered in other courses of this master. We will rather focus on model explainability as this is the main aspect of this course.

## About the data

The dataset we are provided with has the following characteristics : 

- `r nrow(full)` rows (of which `r nrow(train)` for train and `r nrow(test)` for test) and `r ncol(full)` columns.
- The column to predict is : `r y.colname`.
- Number of missing values : `r sum(!complete.cases(full))`

# Exploratory Data Analysis/Inital Modeling

## Correlations

```{r Correlation threshold definition}
cor.threshold <- .7
```


First, let's have a look at the correlations. As the correlation matrix is too big to be displayed, we are only pointing out the correlations that are above `r cor.threshold`.
Should we do this on train_modeled?

```{r High Correlations function}
train.numeric <- train %>% dplyr::select_if(is.numeric)
cor.train <- cor(train.numeric)
cor.df <- as.data.frame(cor.train)

get.high.correlations <- function(cor.matrix, cor.threshold = .7, column.filter = NULL, max.elements = NULL){
  res.df <- which(
    cor.train - lower.tri(
      matrix(
        1, 
        nrow = nrow(cor.train),
        ncol = nrow(cor.train)
      ), 
      diag = TRUE
    ) > cor.threshold,
    arr.ind = TRUE
  ) %>%
    as.data.frame() %>% 
    `colnames<-`(c("c1", "c2")) %>% 
    mutate(
      Column1 = colnames(cor.train)[c1],
      Column2 = colnames(cor.train)[c2]
    ) %>% 
    mutate(
      Correlation = apply(., 1, function(row){
        cor.matrix[
          as.integer(row[which(names(row) == "c1")]), 
          as.integer(row[which(names(row) == "c2")])
        ]
      })
    ) %>% 
    select(Column1, Column2, Correlation) %>% 
    arrange(desc(Correlation))
  
  if(!is.null(column.filter)) res.df <- res.df %>% 
      filter(Column1 == column.filter | Column2 == column.filter)
  if(!is.null(max.elements)) res.df <- res.df %>% top_n(5, Correlation)
  res.df
}
```

```{r Printing tables side by side, results='asis', echo=FALSE}
    # Setting `results = 'asis'` allows for using Latex within the code chunk
    cat('\\begin{figure}')
    cat('\\begin{center}')
    # `{c c}` Creates a two column table
    # Use `{c | c}` if you'd like a line between the tables
    cat('\\begin{tabular}{ c c }')
    print(knitr::kable(
      get.high.correlations(cor.train, cor.threshold), 
      format = "latex"
    ))
    # Separate the two columns with an `&`
    cat('&')
    print(knitr::kable(
      get.high.correlations(cor.train, .01, "SalePrice", 5), 
      format = "latex"
    ))
    cat('\\end{tabular}')
    cat("\\caption{Most correlated columns (all | SalePrice)}")
    cat('\\end{center}')
    cat('\\end{figure}')
```

```{r, fig.align='center'}
corrplot::corrplot(cor(train.numeric))
```
We have way too many features for the number of observations we have.
```{r}
dim(train.modeled)
```


## Factors

An intuitive factor that comes to one's mind when thinking about the price of a house is the _neighbourhood_ in which it is located.

```{r, Neighborhood boxplots, fig.height=4}
graphics::layout(
  matrix(c(1,2), 1, 2, byrow = TRUE),
  widths=c(2,1), 
  heights=c(1,1)
)

boxplot(
  SalePrice ~ Neighborhood, 
  data = full,
  main = "SalePrice per Neighborhood"
)
df.means <- full %>% group_by(Neighborhood) %>% summarise(avg = mean(SalePrice))
vec.means = df.means$avg
names(vec.means) <- df.means$Neighborhood
boxplot(vec.means, main = "Means of Neighborhoods")
```

On the graph on the left, we plot the boxplot of the SalePrice for every Neighborhood. We notice that 3 neighborhood have noticeably higher values than the others. To check that, we use a boxplot of the averages of the SalePrice per Neighborhood and we see that only one value really stands out. The name of the coresponding Neighborhood is `r names(vec.means)[which.max(vec.means)]` 


# Modeling and Diagnostics

```{r Full LM, warning=F}
res.lm.intercept <- lm(SalePrice~1, data = train)
res.full.lm <- lm(SalePrice~., data = train)
par(mfrow = c(2, 2))
plot(res.full.lm, which = c(1, 2, 3))
acf(residuals(res.full.lm), main = "")
```

We can see that the residual line is almost horizontal. We can deduce that postulate stating that the errors are centered is satisfied.

We can observe that the normality hypothesis is not verified when we look at the Q-Q plot, especilally for the tails of the distribution.

The standardized residuals line is not horizontal here, it looks more like a parable. The homoscedasticity postulate is not verified.

On the autocorrelation plot, we observe only one value being over the threshold but it's not high enough to question the uncorrelation of the errors. Thus, we can say that the errors are uncorrelated.

```{r}
library(olsrr)
ols_test_breusch_pagan(res.full.lm)
```

Also the Breusch-Pagan test rejects the homoscedasticity hypothesis.

```{r}
trainlog  <- train %>% mutate(logSalePrice=log(SalePrice)) %>% subset(select= -c(SalePrice))

res.full.log.lm <- lm(logSalePrice~., data = trainlog)
summary(res.full.lm)
par(mfrow = c(2, 2))
plot(res.full.log.lm, which = c(1, 2, 3))
acf(residuals(res.full.log.lm), main = "")
```
The log-transform of the response improves the diagnostic plots:
the third plot shows now some improvement regarding homoscedasticity. We could work with that.

```{r}
ols_test_breusch_pagan(res.full.log.lm) 
```
Problems of Heteroskedasticity:
- The OLS estimators and regression predictions based on them remains unbiased and consistent.
- The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too.
- Because of the inconsistency of the covariance matrix of the estimated regression coefficients, the tests of hypotheses, (t-test, F-test) are no longer valid.

## Outlier analysis.

```{r}
car::influenceIndexPlot(res.full.lm)
```


```{r}
res.outlier.test <- car::outlierTest(res.full.lm, n.max = 20)
```

```{r}
train %>% summarise(avg = mean(SalePrice))
train[as.numeric(names(res.outlier.test$p)), ]
```


```{r Cooks distance, warning=F}
plot(res.full.lm, which = 5)
```


We observe no Cook distance over 1 so we can, at first sight infer that there are no outliers. A more detailed outlier analysis is still necessary to be more confident about that.


```{r Variable selection}
res.backward <- stepAIC(
  res.full.lm,
  ~.,
  trace = F,
  direction=c('backward')
)
nrow(summary(res.backward)$coefficients)
```

```{r}
predict.elsaticnet <- function(train.x, train.y, test.x, test.y, alpha = 0.01, lambda = c(1e5, .5e5, 32200, .25e5, 18000, 1e4, .5e4, 1e3, 1e2, 1e1, 1, 1e-1, 1e-2)){
  
  # Fit
  res.glmnet <- glmnet::glmnet(
    x = train.x,
    y = train.y,
    alpha = 0,
    lambda = lambda
  )
  
  # Prediction
  res.pred <- predict(res.glmnet, newx = test.x)
  colnames(res.pred) <- lambda
  # MSE
  mses <- apply(res.pred, 2, function(col){
    sum((col - test.y)^2)
  })
  names(mses) <- as.character(lambda)
  
  # Return object
  list(
    model = res.glmnet,
    pred = res.pred,
    mse = mses,
    lambda = lambda
  )
}

pred.res <- predict.elsaticnet(
  train.x = train.modeled, 
  train.y = train.y, 
  test.x = test.modeled, 
  test.y = test.y
)
plot(sqrt(pred.res$mse) ~ pred.res$lambda, log = "x", type = "o", main = "RMSE=f(lambda)", xlab="lambda", ylab="RMSE")
```


# Final Models



# Discussion

Uncomment to run, takes around 1 minute to run.

```{r}
res.caret <- caret::train(
  x = train.modeled,
  y = train.y,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = seq(0, 0.03, 0.002),
    lambda = seq(0, 1e5, 1000)
  ),
  metric = "RMSE"
)

```

```{r}
plot(res.caret)
```

```{r}
res.caret$bestTune
```

