---
title: "MAP535 Project"
author: "Leonardo Natale - Guillaume Le Fur"
date: "08/12/2019"
output: pdf_document
---

```{r setup, include=FALSE}
library(plotly)
library(leaps)
library(MASS)
library(caret)
library(dplyr)
knitr::opts_chunk$set(echo = FALSE)
load("data/DataProject.RData")

train.x <- train %>% select(-SalePrice)
train.y <- train %>% select(SalePrice) %>% "$"(SalePrice)

test.x <- test %>% select(-SalePrice)
test.y <- test %>% select(SalePrice) %>% "$"(SalePrice)

full <- train %>% 
  mutate(type = "train") %>%
  rbind(test %>% mutate(type = "test")) %>% 
  transform(type = as.factor(type))


full.modeled <- model.matrix(SalePrice ~ ., data = full)
train.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 1) %>% 
  dplyr::select(-typetrain) %>% 
  as.matrix()
test.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 0) %>%
  select(-typetrain) %>% 
  as.matrix()
```

# Introduction

The following table contains the 5 predictors that are the most correlated to the sales price.

# Exploratory Data Analysis/Inital Modeling

```{r High Correlations}
train.numeric <- train %>% dplyr::select_if(is.numeric)
cor.train <- cor(train.numeric)

get.high.correlations <- function(cor.matrix, cor.threshold = .7){
  which(
    cor.train - lower.tri(
      matrix(
        1, 
        nrow = nrow(cor.train),
        ncol = nrow(cor.train)
      ), 
      diag = TRUE
    ) > cor.threshold,
    arr.ind = TRUE
  ) %>%
  as.data.frame() %>% 
  `colnames<-`(c("c1", "c2")) %>% 
  mutate(
    couple = paste(
      colnames(cor.train)[c1],
      colnames(cor.train)[c2],
      sep = " and "
    )
  ) %>% 
  select(couple) %>% 
  "$"(couple) %>% 
  paste("have a high correlation.")
}

get.high.correlations(cor.train)
```

# Modeling and Diagnostics

```{r Full LM, warning=F}
res.lm.intercept <- lm(SalePrice~1, data = train)
res.full.lm <- lm(SalePrice~., data = train)
par(mfrow = c(2, 3))
plot(res.full.lm)
acf(residuals(res.full.lm), main = "")
```


We can see that the residual line is almost horizontal. We can deduce that postulate stating that the errors are centered is satisfied.

We can observe that the normality hypothesis is not verified when we look at the Q-Q plot, especilally for the tails of the distribution.

The standardized residuals line is not horizontal here, it looks more like a parable. The homoscedasticity postulate is not verified.

On the autocorrelation plot, we observe only one value being over the threshold but it's not high enough to question the uncorrelation of the errors. Thus, we can say that the errors are uncorrelated.

We observe no Cook distance over 1 so we can, at first sight infer that there are no outliers. A more detailed outlier analysis is still necessary to be more confident about that.

## Outlier analysis.

TODO



```{r Variable selection}
cn <- colnames(train)
cn <- cn[cn != "SalePrice"]
res.backward <- stepAIC(
  res.full.lm, 
  ~.,
  trace = F,
  direction=c('backward')
)
nrow(summary(res.backward)$coefficients)
```

```{r}
predict.elsaticnet <- function(train.x, train.y, test.x, test.y, alpha = 0, lambda = c(1e3, 1e2, 1e1, 1, 1e-1, 1e-2)){
  
  # Fit
  res.glmnet <- glmnet::glmnet(
    x = train.x, 
    y = train.y,
    alpha = 0,
    lambda = c(1e3, 1e2, 1e1, 1, 1e-1, 1e-2)
  )
  
  # Prediction
  res.pred <- predict(res.glmnet, newx = test.x)
  
  # MSE
  mses <- apply(res.pred, 2, function(col){
    sum((col - test.y)^2)
  })
  
  # Return object
  list(
    model = res.glmnet,
    pred = res.pred,
    mse = mses
  )
}

pred.res <- predict.elsaticnet(
  train.x = modeled.train, 
  train.y = train.y, 
  test.x = modeled.test, 
  test.y = test.y
)

pred.res$mse
```




# Final Models



# Discussion

```{r}
```

