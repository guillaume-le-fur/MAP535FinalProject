---
title: "MAP535 Project"
author: "Leonardo Natale - Guillaume Le Fur"
date: "Fall 2019"
output: pdf_document
---

```{r Setup chunk, include=FALSE}
library(plotly)
library(leaps)
library(MASS)
library(caret)
library(tibble)
library(dplyr)
library(car)
library(lmtest)
library(forcats)
library(olsrr)
library(randomForest)

knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
load("data/DataProject.RData")
load("heavy_runs.RData")
```

```{r Data Preprocessing}
rmses <- list() # Used to store our different RMSEs along the way.

train <- train %>% column_to_rownames("Id")
test <- test %>% column_to_rownames("Id")

y.colname <- "SalePrice"
train <- train %>% 
  rename(
    "FirstFlrSF" = "1stFlrSF", 
    "SecondFlrSF" = "2ndFlrSF"
  )
test <- test %>% 
  rename(
    "FirstFlrSF" = "1stFlrSF", 
    "SecondFlrSF" = "2ndFlrSF"
  )

full <- train %>% 
  mutate(type = "train") %>% # mutate creates a new variable
  rbind(test %>% mutate(type = "test")) %>% 
  mutate_if(is.factor, as.factor) %>% 
  transform(type = as.factor(type))
old.full.dim <- dim(full)

full.modeled <- model.matrix(SalePrice ~ ., data = full) # creates dummy variables
old.full.modeled.dim <- dim(full.modeled)
```

For this project, we are interested in predicting house prices in Ames, Iowa given several descriptive features.

Our goal is to come up with the best possible model and to master the techniques covered in class by providing a step-by-step solution and analysis of the problem, which could be easily interpretable and adopted as a solid base for future projects.

# Data Exploration

The dataset is composed of: 

- `r nrow(full)` rows (of which `r nrow(train)` for train and `r nrow(test)` for test) and `r ncol(full)` columns.
- The column to predict is : `r y.colname`.
- Number of missing values : `r sum(!complete.cases(full))`

## Feature analysis

```{r SalePrice graphs, fig.height=3}
par(mfrow = c(1,2))

hist(full$SalePrice, breaks = 100, main = "Histogram of SalePrice", xlab = "SalePrice", ylab = "Frequency")
hist(log(full$SalePrice), breaks = 100, main = "Histogram of SalePrice", xlab = "log(SalePrice)", ylab = "Frequency")
# boxplot(full$SalePrice, boxwex = 0.7, main = "", ylab = "SalePrice")
```
The distribution of _SalePrice_ has a highly positive skewness. Plotting the distribution on the log scale transforms our skewed data to approximately conform to normality.

```{r SalePrice Condition scatterplot, fig.height=3}
par(mfrow = c(1,2))
with(full, plot(OverallCond, SalePrice))
with(full, plot(GrLivArea, SalePrice))
```

Intuitively, "Overall Condition" and  "Above Ground Living Area Square Feet" are two important indicators of "Sale Price". The pattern is clear for the latter, while the former is less obvious and there are many high values of _SalePrice_ for average values of _OverallCond_. This could reflect other factors such as the house location.

```{r Neighborhood boxplots, fig.height=3}
graphics::layout(
  matrix(c(1,2), 1, 2, byrow = TRUE),
  widths=c(2,1), 
  heights=c(1,1)
)

boxplot(
  SalePrice ~ Neighborhood, 
  data = full,
  main = "SalePrice per Neighborhood"
)
df.means <- full %>% group_by(Neighborhood) %>% summarise(avg = mean(SalePrice))
vec.means = df.means$avg
names(vec.means) <- df.means$Neighborhood
boxplot(vec.means, main = "Means of Neighborhoods")
```

Another intuitive factor that comes to mind when thinking about the price of a house is its neighbourhood.
The graph on the left shows the boxplot of the _SalePrice_ for every _Neighborhood._ Three neighborhoods have noticeably higher values than the others. The boxplot on the right helps us checking as it plots the averages of the _SalePrice_ per Neighborhood. Only one value really stands out and the name of the coresponding _Neighborhood_ is `r names(vec.means)[which.max(vec.means)]`.

# Data Manipulation

We have noticed how the following categorical columns do not add much value to our data.

```{r Summary of deleted columns}
summary.columns <- c("Utilities", "Street", "Condition2", "RoofMatl", "Exterior2nd")
full.sum <- summary(full)
m <- match(summary.columns, trimws(colnames(full.sum)))
full.sum[, m]
```

The columns _Utilities_, _Street_, _Condition2_ and _RoofMatl_ have been removed as almost all of our observations fall into one particular class. _Exterior2nd_ has been dropped because it presents the same values as _Exterior1st_.

Following a closer look at each feature, we noticed how some factor levels are very underrepresented. This could raise flags when divinding into training and test data: with a random split, some factor levels which appear only a few times, could end all in the test data, resulting in a column of only 1s in our train data.

Our solution consists in gathering more information on the different categories and group them when possible. For instance, "Gable" and "Shed" are very similar kinds of _RoofStyle_ and thus have been encoded into one level.
Levels in factors expressing conditions, have been grouped according to their grade: "Good" and "Excellent" become "Above Average".

A more elegant solution could consist in forcing each category to be represented in both training and test.

```{r Factors relevelling and deleting some features}
full <- full %>% select(-c(Utilities,Street,Condition2,RoofMatl,Exterior2nd))

test.list <- list(
  Exterior1st = list("Shng" = c("AsbShng", "AsphShn"),"Brk" = c("BrkComm", "BrkFace"),"Stucco" = c("ImStucc", "Stucco"),"Other" = c("CBlock", "Stone")),
  LotConfig = list("FR23" = c("FR2", "FR3")),
  LandSlope = list("Mod" = c("Mod", "Sev")),
  Condition1 = list("RRN" = c("RRNn", "RRAn"),"RRE" = c("RRAe", "RRNe")),
  RoofStyle = list("Gable" = c("Gable", "Shed")), # On Wikipedia are sinonyms
  ExterCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")), # BlA = Below Average, AbA= Above Average
  BsmtCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd")),
  Heating = list("FWF" = c("Floor", "Wall")), # Merge wall and floor furnace
  HeatingQC = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")),
  Electrical = list("Standard" = c("SBrkr"),"Other" = c("FuseA", "FuseF", "FuseP", "Mix")),
  GarageQual = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")),
  GarageCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex"))
)
# What about column Functional?
# What about column SaleType and SaleCondition?

# Adapting fct_collapse because it wasn't doing what I wanted.
fct_collapse_perso <- function (.f, ..., group_other = FALSE) 
{
  new <- rlang::dots_list(...)[[1]][[1]]
  levs <- as.list(unlist(new, use.names = FALSE))
  if (group_other) {
    f <- check_factor(.f)
    levels <- levels(f)
    new[["Other"]] <- levels[!levels %in% levs]
    levs <- levels
  }
  names(levs) <- names(new)[rep(seq_along(new), vapply(new, 
    length, integer(1)))]
  fct_recode(.f, !!!levs)
}

# TODO Do it with apply/lapply
for(i in 1:length(test.list)){
  full[[names(test.list)[i]]] <- fct_collapse_perso(full[[names(test.list)[i]]], list(test.list[[i]]))
}
new.full.dim <- dim(full)
```

We also created new features based on aggregations of the response variable by some of the predictors. We computed the average _SalePrice_ per : _Neighborhood_, _LotShape_, _OverallCond_ and _MSSubClass_.

```{r Feature eng reagarding y}
full <- full %>% 
  inner_join(
    train %>% group_by(Neighborhood) %>% summarise(AvgNeighbourSalePrice = mean(SalePrice)),
    by = "Neighborhood"
  ) %>%
  inner_join(
    train %>% group_by(LotShape) %>% summarise(AvgLotShapeSalePrice = mean(SalePrice)),
    by = "LotShape"
  ) %>% 
  inner_join(
    train %>% group_by(OverallCond) %>% summarise(AvgOverallCondSalePrice = mean(SalePrice)),
    by = "OverallCond"
  ) %>% 
    inner_join(
    train %>% group_by(MSSubClass) %>% summarise(AvgMSSubClassSalePrice = mean(SalePrice)),
    by = "MSSubClass"
  )
```


```{r Recreate modeled data}
full.modeled <- model.matrix(SalePrice ~ ., data = full)
new.full.modeled.dim <- dim(full.modeled)

train.modeled <- full.modeled %>%
  as.data.frame() %>% 	  
  filter(typetrain == 1) %>% 	  
  dplyr::select(-typetrain) 

test.modeled <- full.modeled %>%
  as.data.frame() %>% 	  
  filter(typetrain == 0) %>%	 
  select(-typetrain)	  

train <- full %>%
  as.data.frame() %>% 
  filter(type == "train") %>% 
  dplyr::select(-type)

test <- full %>%
  as.data.frame() %>% 
  filter(type == "test") %>% 
  dplyr::select(-type)
```

With this approach, we transition from `r old.full.modeled.dim[2]` to `r new.full.modeled.dim[2]` columns.

```{r Correlation threshold definition}
cor.threshold <- .7
```

But even though this method allowed us to reduce significantly our number of columns, we still have to many columns to plot a correlation matrix. The two tables below show correlations value above `r cor.threshold`. On the right, the features most correlated to _SalePrice_ are displayed.

```{r High Correlations function}
train.numeric <- train %>% dplyr::select_if(is.numeric)
cor.train <- cor(train.numeric)
cor.df <- as.data.frame(cor.train)

get.high.correlations <- function(cor.matrix, cor.threshold = .7, column.filter = NULL, max.elements = NULL){
  res.df <- which(
    cor.train - lower.tri(
      matrix(
        1, 
        nrow = nrow(cor.train),
        ncol = nrow(cor.train)
      ), 
      diag = TRUE
    ) > cor.threshold,
    arr.ind = TRUE
  ) %>%
    as.data.frame() %>% 
    `colnames<-`(c("c1", "c2")) %>% 
    mutate(
      Column1 = colnames(cor.train)[c1],
      Column2 = colnames(cor.train)[c2]
    ) %>% 
    mutate(
      Correlation = apply(., 1, function(row){
        cor.matrix[
          as.integer(row[which(names(row) == "c1")]), 
          as.integer(row[which(names(row) == "c2")])
        ]
      })
    ) %>% 
    select(Column1, Column2, Correlation) %>% 
    arrange(desc(Correlation))
  
  if(!is.null(column.filter)) res.df <- res.df %>% 
      filter(Column1 == column.filter | Column2 == column.filter)
  if(!is.null(max.elements)) res.df <- res.df %>% top_n(5, Correlation)
  res.df
}
```

```{r Printing tables side by side, results='asis', echo=FALSE}
    # Setting `results = 'asis'` allows for using Latex within the code chunk
    cat('\\begin{figure}')
    cat('\\begin{center}')
    # `{c c}` Creates a two column table
    # Use `{c | c}` if you'd like a line between the tables
    cat('\\begin{tabular}{ c c }')
    print(knitr::kable(
      get.high.correlations(cor.train, cor.threshold), 
      format = "latex"
    ))
    # Separate the two columns with an `&`
    cat('&')
    print(knitr::kable(
      get.high.correlations(cor.train, .01, "SalePrice", 5), 
      format = "latex"
    ))
    cat('\\end{tabular}')
    cat("\\caption{Most correlated columns (all | SalePrice)}")
    cat('\\end{center}')
    cat('\\end{figure}')
```

It is very interesting to see how _GarageArea_ and _GarageCars_ are important to Americans when determing house prices. The _OverallQual_ is also very considered, together with the _GrLivArea_. 

We still have a high number of features for the number of observations(`r dim(train.modeled)`). We will later perform some variable selection to reduce the number of predictors.

# Modeling and Diagnostics

We start by fitting a linear model and try to validate the hypothesis inherent to the linear model.

```{r Full LM, warning=F}
res.lm.intercept <- lm(SalePrice~1, data = train.modeled %>% cbind(train %>% select(SalePrice)))
res.full.lm <- lm(SalePrice~., data = train.modeled %>% cbind(train %>% select(SalePrice)))
par(mfrow = c(2, 2))
plot(res.full.lm, which = c(1, 2, 3))
acf(residuals(res.full.lm), main = "")

pred <- predict(
  res.full.lm,
  newdata = test.modeled %>%
    as.data.frame()
)
rmses$Linear.model <- sqrt(mean((pred - test$SalePrice)^2))
```

P1) In the Residual vs Fitted plot, the residual line is almost horizontal at zero. Thus, we can deduce that centered errors postulate is met.

P2) The homoscedasticity postulate is not verified as the standardized residuals line is not horizontal with equally spread points. It looks more like a parable. Also, the p-value of the _Breusch-Pagan_ test is `r ols_test_breusch_pagan(res.full.lm)$p`, rejecting the homoskedasticity hypothesis.

P3) On the autocorrelation plot, only one value is over the threshold but it is not high enough to question the uncorrelation of the errors. The _DurbinWatsonTest_ returns a p-value of `r durbinWatsonTest(res.full.lm)$p` and confirms that the errors are uncorrelated.

P4) The normality hypothesis is not verified when we look at the Q-Q plot, especially for the tails of the distribution. The result is confirmed by the _ShapiroTest_, which return a p-value of `r shapiro.test(residuals(res.full.lm))$p`.


In order to try to fix, or at least minimize, the heteroskedasticity of the data, we could try to take the log of the response variable.

```{r Logarithmic fit}
trainlog  <- train.modeled %>%
  as.data.frame() %>% 
  cbind(data.frame(SalePrice = train$SalePrice)) %>% 
  mutate(logSalePrice=log(SalePrice)) %>% 
  select(-SalePrice)

res.full.log.lm <- lm(logSalePrice~., data = trainlog)
```

```{r, warning = F}
pred <- predict(
  res.full.log.lm,
  newdata = test.modeled %>%
    as.data.frame()
)
rmses$logarithm.lm <- sqrt(mean((exp(pred) - test$SalePrice)^2))
```

The log-transform of the response improves the diagnostic plots:

```{r, warning = F}
par(mfrow = c(2, 2))
plot(res.full.log.lm, which = c(1, 2, 3))
acf(residuals(res.full.log.lm), main = "")
```

The third plot shows now some improvement regarding homoscedasticity and we could work with this model instead. However, it is important to know th p-value of the _Breusch-Pagan_ test on this model is `r ols_test_breusch_pagan(res.full.log.lm)$p`. It is a bit higher but we still reject the homoskedasticity hipothesis.

The issues raised by heteroskedasticity are the following:

- The OLS estimators and regression predictions based on them remains unbiased and consistent.
- The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too.  
<!-- - Because of the inconsistency of the covariance matrix of the estimated regression coefficients, the tests of hypotheses, (t-test, F-test) are no longer valid. -->

### Outlier analysis

```{r Cooks distance, warning=F, fig.height=4}
plot(res.full.lm, which = 5)
```
We observe points with Cook distance greater than 1 so we can, at first sight, infer that there are no outliers. However, a more detailed outlier analysis is still necessary.

```{r InfluenceIndexPlot}
car::influenceIndexPlot(res.full.lm, vars=c("Studentized", "hat"))
```

The first plot makes it possible to identify outliers, many observations seem doubtful. The second plot shows the presence of many leverage points (with hate values above 0.5).

```{r, OutlierTest}
res.outlier.test <- car::outlierTest(res.full.lm, n.max = 20)
outliers.index <- paste(names(res.outlier.test$p), collapse = ", ")
```

We consider necessary to perform a Bonferroni _OutlierTest_ to identify outliers. The following points are labeled as such by the test: `r outliers.index`.

After analysis, it seems that this points are not really outliers because half of them are huge houses in the NoRidge Heighborhood. They are atypical point rather than outliers. We decide not to remove them in the end. 

## Model Tuning

### Variable Selection

We use a stepwise method with the BIC criterion to determine the best model to use.

```{r Variable selection, eval=F, include = F}
res.backward <- stepAIC(
  res.full.log.lm,
  SalePrice~1,
  trace = F,
  k = log(nrow(train)),
  direction=c('backward')
)
```

```{r, warning = F}
bic.kept.columns <- rownames(summary(res.backward)$coefficients)
nb.bic.columns <- length(bic.kept.columns)
bic.lm.res <- lm(
  SalePrice~.,
  data = train.modeled %>%
    as.data.frame() %>% 
    select(bic.kept.columns) %>%
    cbind(data.frame(SalePrice = train$SalePrice))
)
pred <- predict(
  bic.lm.res, 
  newdata = test.modeled[, rownames(summary(res.backward)$coefficients)] %>% as.data.frame()
)
rmses$BIC.model <- sqrt(mean((pred - test$SalePrice)^2))
```

```{r}
as.data.frame(summary(res.backward)$coefficients) %>%
  rownames_to_column("colname") %>%
  rename("prob" = "Pr(>|t|)") %>%
  arrange(prob) %>%
  slice(1:15) %>%
  column_to_rownames("colname") %>% 
  select(prob) %>% 
  knitr::kable(digits = 150, caption = "Most significative columns according to the BIC criterion")
```

We can see that the most significative predictors correspond to reasonable criteria when estimating the price of a house (surface, neighborhood, Garage, Year, etc.).

The number of columns that our BIC models have is `r nb.bic.columns`, which is much fewer than our previous models.

### Hyper-parameter tuning

We try to fit an ElasticNet model and tune its hyperparameters ($\lambda$ and $\alpha$). using `caret::train`

After some tests, we use the following intervals for the gridsearch :

- $\alpha \in [0, 0.03]$ - step : 0.002
- $\lambda \in [0, 1.10^5]$ - step : 1000

```{r HyperParameter Tuning, warning = F, eval = F, include = F}
res.caret <- caret::train(
  x = train.modeled,
  y = train$SalePrice,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = seq(0, 0.03, 0.002),
    lambda = seq(0, 1e5, 1000)
  ),
  metric = "RMSE"
)
```

```{r}
best.alpha <- res.caret$bestTune$alpha
best.lambda <- res.caret$bestTune$lambda
```


```{r, Plot of res, fig.height=4, fig.width=6}
plot(res.caret)
```

The optimal values are $\alpha$ = `r best.alpha` and $\lambda$ = `r best.lambda`.

```{r, Optimal Parameters}
optimal.model <- glmnet::glmnet(
  x = train.modeled %>% as.matrix(), 
  y = train$SalePrice, 
  alpha = res.caret$bestTune$alpha, 
  lambda = res.caret$bestTune$lambda
)
res.pred.glmnet <- predict(optimal.model, newx = test.modeled %>% as.matrix()) - test$SalePrice
rmses$optimal.glmnet <- sqrt(mean(res.pred.glmnet^2))
```

### Anova

We also run an Analysis of Variance to determine the importance of certain factors on the data.

```{r}
anova.full <- anova(res.full.lm)
as.data.frame(anova.full) %>% 
  rownames_to_column("rown") %>% 
  rename("prob" = "Pr(>F)") %>% 
  filter(prob < 0.001) %>% 
  select(prob, rown) %>% 
  arrange(prob) %>% 
  slice(1:5) %>% 
  column_to_rownames("rown") %>% 
  knitr::kable(caption = "Five most significative columns according to Anova", digits = 500)
```

The output of the Anova makes a lot of sense. Indeed, the most signifcant variables are the ones that comes to one's mind when estimating the price of a house : Type of building sold (MSSubClass), Neighborhood where the place is located, Overall quality of the good, the area of the place and the linear feet of street connected to property.

\newpage

# Final Models

```{r}
random.forest.res <- randomForest::randomForest(
  x = train.modeled, 
  y = train$SalePrice, 
  xtest = test.modeled, 
  ytest = test$SalePrice
)

rmses$random.forests <- sqrt(tail(random.forest.res$test$mse, n = 1))
```

```{r}
gbm.res <- gbm::gbm(
  SalePrice ~ ., 
  data = train,
  distribution = "gaussian",
  n.trees = 500,
  interaction.depth	= 2,
  n.minobsinnode = 10,
  cv.folds = 5
)
pred <- predict(gbm.res, newdata = test %>% select(-SalePrice), n.trees = 500) - test$SalePrice
rmses$gradient.boosting <- sqrt(mean(pred^2))
```

```{r}
xgboost.res <- xgboost::xgboost(
  data = train.modeled %>% as.matrix(),
  label = train$SalePrice,
  nrounds = 100,
  verbose = 0
)
pred <- predict(xgboost.res, newdata = test.modeled %>% as.matrix()) - test$SalePrice
rmses$xgboost <- sqrt(mean(pred^2))
```


```{r}
data.frame(
  Model = names(rmses),
  RMSE = unlist(rmses)) %>% 
  mutate(Model = tools::toTitleCase(stringr::str_replace(Model, "\\.", " "))) %>% 
  column_to_rownames("Model") %>% 
  knitr::kable(caption = "Summary of model performance.")
```

We have tried several models that were not considered in this course to be able to compare the models that we have built and analysed in this document.
We can see that the linear model is not performing that bad given the difference of RMSE it has with more elaborated models.

The model obtained with the BIC criterion doesn't have a really good performance, mostly because it has a reduced number of columns. If we had had more data, maybe we could have kept more columns and come out with a better model using the BIC criterion. The data produced by BIC could be useful if we try to fit complicated models because the training time would be smaller given the small number of predictors.

We can also notice that penalization doesn't make ou model better, which can look surprising.

Because they are not being considered in this course, we do not keep random forests or gradient boosting models as our final model, we choose to keep the linear model with the log of the response variable as our final model because it is a good compromise between explainability and perfromance.

# Conclusion

This project was a good opportunity to put what we have learnt this semester into practice and also to become more at ease with R. Rather than a pure data science project, aiming at having the model with the best score, it was interesting to focus on other important aspects such as outlier analysis or variable selection.
We have built a model that is pretty robust, which does not contain too many predictors and that has a satisfying RMSE.

It would have been nice to have more rows in the data set to be able to obtain more robust models.
