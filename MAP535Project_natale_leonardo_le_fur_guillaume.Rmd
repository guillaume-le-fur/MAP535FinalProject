---
title: "MAP535 Project"
author: "Leonardo Natale - Guillaume Le Fur"
date: "08/12/2019"
output: pdf_document
---

```{r Setup chunk, include=FALSE}
library(plotly)
library(leaps)
library(MASS)
library(caret)
library(dplyr)
library(car)
library(lmtest)
library(forcats)

knitr::opts_chunk$set(echo = FALSE)
load("data/DataProject.RData")

row.names(train) <- train$Id
train <- subset(train,select=c(-Id))
row.names(test) <- test$Id
test <- subset(test,select=c(-Id))

y.colname <- "SalePrice"
train.y <- train %>% select(SalePrice) %>% "$"(SalePrice)
train.x <- train %>% select(-SalePrice)

test.x <- test %>% select(-SalePrice)
test.y <- test %>% select(SalePrice) %>% "$"(SalePrice)

full <- train %>% 
  mutate(type = "train") %>% # mutate creates a new variable
  rbind(test %>% mutate(type = "test")) %>% 
  transform(type = as.factor(type))
old.full.dim <- dim(full)

full.modeled <- model.matrix(SalePrice ~ ., data = full) # creates dummy variables
old.full.modeled.dim <- dim(full.modeled)

train.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 1) %>% 
  dplyr::select(-typetrain) %>% 
  as.matrix()
test.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 0) %>%
  select(-typetrain) %>% 
  as.matrix()
```



# Introduction

## Purpose of this document

The aim of this document is to summarise and apply what has been covered during the MAP535 course on a real case study. The case study we are interested in is the prediction of the price of a house in the city of Ames, Iowa, given a certain number of predictors.
Even though the aim of such projects is to give the best possible model for prediction, we chose not to focus on this part because this is a part of data science which is already covered in other courses of this master. We will rather focus on model explainability as this is the main aspect of this course.

## About the data

The dataset we are provided with has the following characteristics : 

- `r nrow(full)` rows (of which `r nrow(train)` for train and `r nrow(test)` for test) and `r ncol(full)` columns.
- The column to predict is : `r y.colname`.
- Number of missing values : `r sum(!complete.cases(full))`

# Exploratory Data Analysis/Inital Modeling

## Feature engineering

### Column removal

We notice that some categorical columns are not really useful.

```{r Summary of deleted columns}
summary.columns <- c("Utilities", "Street", "Condition2", "RoofMatl", "Exterior2nd")
full.sum <- summary(full)
m <- match(summary.columns, trimws(colnames(full.sum)))
full.sum[, m]
```

We remvove Utilities, Street, Condition 2 and RoofMatl because it is not relevant, given the repartition of the levels of the factors. We remove Exterior2nd because it is always the same values as Exterior1st.

### Attempt to reduce categories by grouping

```{r Factors relevelling and deleting some features}
# Condensed version
full <- full %>% select(-c(Utilities,Street,Condition2,RoofMatl,Exterior2nd))
test.list <- list(
  Exterior1st = list("Shng" = c("AsbShng", "AsphShn"),"Brk" = c("BrkComm", "BrkFace"),"Stucco" = c("ImStucc", "Stucco"),"Other" = c("CBlock", "Stone")),
  LotConfig = list("FR23" = c("FR2", "FR3")),
  LandSlope = list("Mod" = c("Mod", "Sev")),
  Condition1 = list("RRN" = c("RRNn", "RRAn"),"RRE" = c("RRAe", "RRNe")),
  RoofStyle = list("Gable" = c("Gable", "Shed")), # On Wikipedia are sinonyms
  ExterCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")), # BlA = Below Average, AbA= Above Average
  BsmtCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd")),
  Heating = list("FWF" = c("Floor", "Wall")), # Merge wall and floor furnace
  HeatingQC = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")),
  Electrical = list("Standard" = c("SBrkr"),"Other" = c("FuseA", "FuseF", "FuseP", "Mix")), # Not sure about this one
  GarageQual = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")),
  GarageCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex"))
)
# What about column Functional?
# What about column SaleType and SaleCondition?

# Adapting fct_collapse because it wasn't doing what I wanted.
fct_collapse_perso <- function (.f, ..., group_other = FALSE) 
{
  new <- rlang::dots_list(...)[[1]][[1]]
  levs <- as.list(unlist(new, use.names = FALSE))
  if (group_other) {
    f <- check_factor(.f)
    levels <- levels(f)
    new[["Other"]] <- levels[!levels %in% levs]
    levs <- levels
  }
  names(levs) <- names(new)[rep(seq_along(new), vapply(new, 
    length, integer(1)))]
  fct_recode(.f, !!!levs)
}

# TODO find a way to do it with apply/lapply
for(i in 1:length(test.list)){
  full[[names(test.list)[i]]] <- fct_collapse_perso(full[[names(test.list)[i]]], list(test.list[[i]]))
}
new.full.dim <- dim(full)
```

We also changed the levels of the following columns to make them more homogeneous : `r names(test.list)`

```{r Recreate modeled data}
full.modeled <- model.matrix(SalePrice ~ ., data = full) # creates dummy variables
new.full.modeled.dim <- dim(full.modeled)
train.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 1) %>% 
  dplyr::select(-typetrain) %>% 
  as.matrix()

test.modeled <- full.modeled %>%
  as.data.frame() %>% 
  filter(typetrain == 0) %>%
  select(-typetrain) %>% 
  as.matrix()

train <- full %>%
  as.data.frame() %>% 
  filter(type == "train") %>% 
  dplyr::select(-type)

test <- full %>%
  as.data.frame() %>% 
  filter(type == "test") %>% 
  dplyr::select(-type)
```

As we removed 5 columns, there is a bit less columns in the full data than before. But the most striking difference comes from the relevelling of the factors. On the dummified data, we went from `r old.full.modeled.dim[2]` to `r new.full.modeled.dim[2]` columns.


## Correlations

```{r Correlation threshold definition}
cor.threshold <- .7
```


First, let's have a look at the correlations. As the correlation matrix is too big to be displayed, we are only pointing out the correlations that are above `r cor.threshold`.
Should we do this on train_modeled?

```{r High Correlations function}
train.numeric <- train %>% dplyr::select_if(is.numeric)
cor.train <- cor(train.numeric)
cor.df <- as.data.frame(cor.train)

get.high.correlations <- function(cor.matrix, cor.threshold = .7, column.filter = NULL, max.elements = NULL){
  res.df <- which(
    cor.train - lower.tri(
      matrix(
        1, 
        nrow = nrow(cor.train),
        ncol = nrow(cor.train)
      ), 
      diag = TRUE
    ) > cor.threshold,
    arr.ind = TRUE
  ) %>%
    as.data.frame() %>% 
    `colnames<-`(c("c1", "c2")) %>% 
    mutate(
      Column1 = colnames(cor.train)[c1],
      Column2 = colnames(cor.train)[c2]
    ) %>% 
    mutate(
      Correlation = apply(., 1, function(row){
        cor.matrix[
          as.integer(row[which(names(row) == "c1")]), 
          as.integer(row[which(names(row) == "c2")])
        ]
      })
    ) %>% 
    select(Column1, Column2, Correlation) %>% 
    arrange(desc(Correlation))
  
  if(!is.null(column.filter)) res.df <- res.df %>% 
      filter(Column1 == column.filter | Column2 == column.filter)
  if(!is.null(max.elements)) res.df <- res.df %>% top_n(5, Correlation)
  res.df
}
```

```{r Printing tables side by side, results='asis', echo=FALSE}
    # Setting `results = 'asis'` allows for using Latex within the code chunk
    cat('\\begin{figure}')
    cat('\\begin{center}')
    # `{c c}` Creates a two column table
    # Use `{c | c}` if you'd like a line between the tables
    cat('\\begin{tabular}{ c c }')
    print(knitr::kable(
      get.high.correlations(cor.train, cor.threshold), 
      format = "latex"
    ))
    # Separate the two columns with an `&`
    cat('&')
    print(knitr::kable(
      get.high.correlations(cor.train, .01, "SalePrice", 5), 
      format = "latex"
    ))
    cat('\\end{tabular}')
    cat("\\caption{Most correlated columns (all | SalePrice)}")
    cat('\\end{center}')
    cat('\\end{figure}')
```

```{r, fig.align='center'}
corrplot::corrplot(cor(train.numeric))
```

We have way too many features for the number of observations we have.

```{r}
dim(train.modeled)
```


## Factors

An intuitive factor that comes to one's mind when thinking about the price of a house is the _neighbourhood_ in which it is located.

```{r Neighborhood boxplots, fig.height=4}
graphics::layout(
  matrix(c(1,2), 1, 2, byrow = TRUE),
  widths=c(2,1), 
  heights=c(1,1)
)

boxplot(
  SalePrice ~ Neighborhood, 
  data = full,
  main = "SalePrice per Neighborhood"
)
df.means <- full %>% group_by(Neighborhood) %>% summarise(avg = mean(SalePrice))
vec.means = df.means$avg
names(vec.means) <- df.means$Neighborhood
boxplot(vec.means, main = "Means of Neighborhoods")
```

On the graph on the left, we plot the boxplot of the SalePrice for every Neighborhood. We notice that 3 neighborhood have noticeably higher values than the others. To check that, we use a boxplot of the averages of the SalePrice per Neighborhood and we see that only one value really stands out. The name of the coresponding Neighborhood is `r names(vec.means)[which.max(vec.means)]` 


# Modeling and Diagnostics

```{r Full LM, warning=F}
res.lm.intercept <- lm(SalePrice~1, data = train)
res.full.lm <- lm(SalePrice~., data = train)
par(mfrow = c(2, 2))
plot(res.full.lm, which = c(1, 2, 3))
acf(residuals(res.full.lm), main = "")
```

We can see that the residual line is almost horizontal. We can deduce that postulate stating that the errors are centered is satisfied.

We can observe that the normality hypothesis is not verified when we look at the Q-Q plot, especilally for the tails of the distribution.

The standardized residuals line is not horizontal here, it looks more like a parable. The homoscedasticity postulate is not verified.

On the autocorrelation plot, we observe only one value being over the threshold but it's not high enough to question the uncorrelation of the errors. Thus, we can say that the errors are uncorrelated.

```{r Breusch Pagan}
library(olsrr)
ols_test_breusch_pagan(res.full.lm)
```

Also the Breusch-Pagan test rejects the homoscedasticity hypothesis.

```{r Logarithmic fit}
trainlog  <- train %>% mutate(logSalePrice=log(SalePrice)) %>% subset(select= -c(SalePrice))

res.full.log.lm <- lm(logSalePrice~., data = trainlog)
summary(res.full.lm)
par(mfrow = c(2, 2))
plot(res.full.log.lm, which = c(1, 2, 3))
acf(residuals(res.full.log.lm), main = "")
```
The log-transform of the response improves the diagnostic plots:
the third plot shows now some improvement regarding homoscedasticity. We could work with that.

```{r Breausch Pagan 2}
ols_test_breusch_pagan(res.full.log.lm) 
```
Problems of Heteroskedasticity:
- The OLS estimators and regression predictions based on them remains unbiased and consistent.
- The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too.
- Because of the inconsistency of the covariance matrix of the estimated regression coefficients, the tests of hypotheses, (t-test, F-test) are no longer valid.

## Outlier analysis.

```{r InfluenceIndexPlot}
# car::influenceIndexPlot(res.full.lm)
```


```{r, OutlierTest}
res.outlier.test <- car::outlierTest(res.full.lm, n.max = 20)
```

```{r, Outlier Data}
train %>% summarise(avg = mean(SalePrice))
train[as.numeric(names(res.outlier.test$p)), ]
```


```{r Cooks distance, warning=F}
plot(res.full.lm, which = 5)
```


We observe no Cook distance over 1 so we can, at first sight infer that there are no outliers. A more detailed outlier analysis is still necessary to be more confident about that.


```{r Variable selection}
# res.backward <- stepAIC(
#   res.full.lm,
#   ~.,
#   trace = F,
#   direction=c('backward')
# )
# nrow(summary(res.backward)$coefficients)
```

```{r, elasticNetFit}
predict.elsaticnet <- function(train.x, train.y, test.x, test.y, alpha = 0.01, lambda = c(1e5, .5e5, 32200, .25e5, 18000, 1e4, .5e4, 1e3, 1e2, 1e1, 1, 1e-1, 1e-2)){
  
  # Fit
  res.glmnet <- glmnet::glmnet(
    x = train.x,
    y = train.y,
    alpha = 0,
    lambda = lambda
  )
  
  # Prediction
  res.pred <- predict(res.glmnet, newx = test.x)
  colnames(res.pred) <- lambda
  # MSE
  mses <- apply(res.pred, 2, function(col){
    sum((col - test.y)^2)
  })
  names(mses) <- as.character(lambda)
  
  # Return object
  list(
    model = res.glmnet,
    pred = res.pred,
    mse = mses,
    lambda = lambda
  )
}

pred.res <- predict.elsaticnet(
  train.x = train.modeled, 
  train.y = train.y, 
  test.x = test.modeled, 
  test.y = test.y
)
plot(sqrt(pred.res$mse) ~ pred.res$lambda, log = "x", type = "o", main = "RMSE=f(lambda)", xlab="lambda", ylab="RMSE")
```


# Final Models

# Discussion

Uncomment to run, takes around 30 seconds to run.

```{r, HyperParameter Tuning}
# res.caret <- caret::train(
#   x = train.modeled,
#   y = train.y,
#   method = "glmnet",
#   tuneGrid = expand.grid(
#     alpha = seq(0, 0.03, 0.002),
#     lambda = seq(0, 1e5, 1000)
#   ),
#   metric = "RMSE"
# )
```

```{r, Plot of res}
# plot(res.caret)
```

```{r, Optimal Parameters}
# res.caret$bestTune
```

