---
title: "MAP535 Project"
author: "Leonardo Natale - Guillaume Le Fur"
date: "Fall 2019"
output: pdf_document
---

```{r Setup chunk, include=FALSE}
library(plotly)
library(leaps)
library(MASS)
library(caret)
library(tibble)
library(dplyr)
library(car)
library(lmtest)
library(forcats)
library(olsrr)

knitr::opts_chunk$set(echo = FALSE)
load("data/DataProject.RData")
```

```{r Data Preprocessing}
rmses <- list() #Used to store our different RMSEs along the way.

train <- train %>% column_to_rownames("Id")
test <- test %>% column_to_rownames("Id")

y.colname <- "SalePrice"
train <- train %>% 
  rename(
    "FirstFlrSF" = "1stFlrSF", 
    "SecondFlrSF" = "2ndFlrSF"
  )
test <- test %>% 
  rename(
    "FirstFlrSF" = "1stFlrSF", 
    "SecondFlrSF" = "2ndFlrSF"
  )

full <- train %>% 
  mutate(type = "train") %>% # mutate creates a new variable
  rbind(test %>% mutate(type = "test")) %>% 
  mutate_if(is.factor, as.factor) %>% 
  transform(type = as.factor(type))
old.full.dim <- dim(full)

model.data <- function(full){
  train <- full %>%
    as.data.frame() %>% 
    filter(type == "train") %>% 
    dplyr::select(-type)
  
  test <- full %>%
    as.data.frame() %>% 
    filter(type == "test") %>% 
    dplyr::select(-type)

  train.y <- train %>% 
    select(SalePrice) %>%
    "$"(SalePrice)
  train.x <- train %>%
    select(-SalePrice)
  
  test.x <- test %>% 
    select(-SalePrice)
  test.y <- test %>% 
    select(SalePrice) %>% 
    "$"(SalePrice) 

  full.modeled <- model.matrix(SalePrice ~ ., data = full) # creates dummy variables
  train.modeled <- full.modeled %>%
    as.data.frame() %>% 
    filter(typetrain == 1) %>% 
    dplyr::select(-typetrain) %>% 
    as.matrix()
  test.modeled <- full.modeled %>%
    as.data.frame() %>% 
    filter(typetrain == 0) %>%
    select(-typetrain) %>% 
    as.matrix()
  list(
    train = train,
    train.x = train.x,
    train.y = train.y,
    test = test,
    test.x = test.x,
    test.y = test.y,
    full.modeled = full.modeled,
    train.modeled = train.modeled,
    test.modeled = test.modeled
  )
}

modeled.data <- model.data(full)

full.modeled <- modeled.data$full.modeled
old.full.modeled.dim <- dim(full.modeled)

# Some are rendundant here but it's just to be estra sure that
# the factor levels are the good ones.
train.modeled <- modeled.data$train.modeled
test.modeled <- modeled.data$test.modeled
train <- modeled.data$train
test <- modeled.data$test
train.x <- modeled.data$train.x
train.y <- modeled.data$train.y
test <- modeled.data$test
test.x <- modeled.data$test.x
test.y <- modeled.data$test.y
```


We are interested in predicting house prices in Ames, Iowa given several descriptive features.

Our goal is to come up with the best possible model and to master the techniques covered in class by providing a step-by-step solution which could be easily interpretable and adopted as a solid base for future projects.     

# Data Exploration

The dataset is composed of: 

- `r nrow(full)` rows (of which `r nrow(train)` for train and `r nrow(test)` for test) and `r ncol(full)` columns.
- The column to predict is : `r y.colname`.
- Number of missing values : `r sum(!complete.cases(full))`

```{r SalePrice graphs, fig.height=4}
par(mfrow = c(1,2))

hist(full$SalePrice, breaks = 100, main = "Histogram of SalePrice", xlab = "SalePrice", ylab = "Frequency")
hist(log(full$SalePrice), breaks = 100, main = "Histogram of SalePrice", xlab = "log(SalePrice)", ylab = "Frequency")
# boxplot(full$SalePrice, boxwex = 0.7, main = "", ylab = "SalePrice")
```
The distribution of _SalePrice_ has a highly positive skewness. Plotting the distribution on the log scale transforms our skewed data to approximately conform to normality

```{r SalePrice Condition scatterplot, fig.height=4}
par(mfrow = c(1,2))
with(full, plot(OverallCond, SalePrice))
with(full, plot(GrLivArea, SalePrice))
```
Intuitively, "Overall Condition" and  "Above Ground Living Area Square Feet" are two important indicators of "Sale Price". The pattern is clear for the latter, while the former is less obvious and there are many high values of _SalePrice_ for average values of _OveralCond_. This could reflects other factor such as the house location.

```{r Neighborhood boxplots, fig.height=4}
graphics::layout(
  matrix(c(1,2), 1, 2, byrow = TRUE),
  widths=c(2,1), 
  heights=c(1,1)
)

boxplot(
  SalePrice ~ Neighborhood, 
  data = full,
  main = "SalePrice per Neighborhood"
)
df.means <- full %>% group_by(Neighborhood) %>% summarise(avg = mean(SalePrice))
vec.means = df.means$avg
names(vec.means) <- df.means$Neighborhood
boxplot(vec.means, main = "Means of Neighborhoods")
```
Another intuitive factor that comes to mind when thinking about the price of a house is its neighbourhood.
The graph on the left shows the boxplot of the SalePrice for every Neighborhood. Three neighborhood have noticeably higher values than the others. The boxplot on the right helps us checking as it plots the averages of the SalePrice per Neighborhood. Only one value really stands out and the name of the coresponding Neighborhood is `r names(vec.means)[which.max(vec.means)]`.

# Data Manipulation

We have noticed how the following categorical columns do not add much value to our data.
```{r Summary of deleted columns}
summary.columns <- c("Utilities", "Street", "Condition2", "RoofMatl", "Exterior2nd")
full.sum <- summary(full)
m <- match(summary.columns, trimws(colnames(full.sum)))
full.sum[, m]
```

The columns _Utilities_, _Street_, _Condition2_ and _RoofMatl_ have been removed as almost all of our observations fall into one particular class. _Exterior2nd_ has been dropped because it presents the same values as _Exterior1st_.

Following a closer look at each feature, we noticed how some factor levels are very underrepresented. This could raise flags when divinding into training and test data: with a random split, some factor levels which appear only a few times, could end all in the test data, resulting in a column of only 1s in our train data.  

Our solution consists in gathering more information on the different categories and group them when possible. For instance, "Gable" and "Shed" are very similar kinds of _RoofStyle_ and thus have been encoded into one level.
Levels in factors expressing conditions, have been grouped according to their grade: "Good" and "Excellent" become "Above Average".

A more elegant solution could consist in forcing each category to be represented in both training and test.
```{r Factors relevelling and deleting some features}
full <- full %>% select(-c(Utilities,Street,Condition2,RoofMatl,Exterior2nd))
train <- train %>% select(-c(Utilities,Street,Condition2,RoofMatl,Exterior2nd))
test <- test %>% select(-c(Utilities,Street,Condition2,RoofMatl,Exterior2nd))

test.list <- list(
  Exterior1st = list("Shng" = c("AsbShng", "AsphShn"),"Brk" = c("BrkComm", "BrkFace"),"Stucco" = c("ImStucc", "Stucco"),"Other" = c("CBlock", "Stone")),
  LotConfig = list("FR23" = c("FR2", "FR3")),
  LandSlope = list("Mod" = c("Mod", "Sev")),
  Condition1 = list("RRN" = c("RRNn", "RRAn"),"RRE" = c("RRAe", "RRNe")),
  RoofStyle = list("Gable" = c("Gable", "Shed")), # On Wikipedia are sinonyms
  ExterCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")), # BlA = Below Average, AbA= Above Average
  BsmtCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd")),
  Heating = list("FWF" = c("Floor", "Wall")), # Merge wall and floor furnace
  HeatingQC = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")),
  Electrical = list("Standard" = c("SBrkr"),"Other" = c("FuseA", "FuseF", "FuseP", "Mix")),
  GarageQual = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex")),
  GarageCond = list("BlA" = c("Fa", "Po"),"AbA" = c("Gd", "Ex"))
)
# What about column Functional?
# What about column SaleType and SaleCondition?

# Adapting fct_collapse because it wasn't doing what I wanted.
fct_collapse_perso <- function (.f, ..., group_other = FALSE) 
{
  new <- rlang::dots_list(...)[[1]][[1]]
  levs <- as.list(unlist(new, use.names = FALSE))
  if (group_other) {
    f <- check_factor(.f)
    levels <- levels(f)
    new[["Other"]] <- levels[!levels %in% levs]
    levs <- levels
  }
  names(levs) <- names(new)[rep(seq_along(new), vapply(new, 
    length, integer(1)))]
  fct_recode(.f, !!!levs)
}

# TODO Do it with apply/lapply
for(i in 1:length(test.list)){
  full[[names(test.list)[i]]] <- fct_collapse_perso(full[[names(test.list)[i]]], list(test.list[[i]]))
}
new.full.dim <- dim(full)
```

```{r Feature eng reagarding y}
full <- full %>% 
  inner_join(
    train %>% group_by(Neighborhood) %>% summarise(AvgNeighbourSalePrice = mean(SalePrice)),
    by = "Neighborhood"
  ) %>%
  inner_join(
    train %>% group_by(LotShape) %>% summarise(AvgLotShapeSalePrice = mean(SalePrice)),
    by = "LotShape"
  ) %>% 
  inner_join(
    train %>% group_by(OverallCond) %>% summarise(AvgOverallCondSalePrice = mean(SalePrice)),
    by = "OverallCond"
  ) %>% 
    inner_join(
    train %>% group_by(MSSubClass) %>% summarise(AvgMSSubClassSalePrice = mean(SalePrice)),
    by = "MSSubClass"
  )
```

```{r Recreate modeled data}
new.modeled.data <- model.data(full)

full.modeled <- new.modeled.data$full.modeled
new.full.modeled.dim <- dim(full.modeled)

train.modeled <- new.modeled.data$train.modeled
test.modeled <- new.modeled.data$test.modeled

train <- new.modeled.data$train
test <- new.modeled.data$test
train.x <- new.modeled.data$train.x
train.y <- new.modeled.data$train.y
test <- new.modeled.data$test
test.x <- new.modeled.data$test.x
test.y <- new.modeled.data$test.y
```

With this approach, we transition from `r old.full.modeled.dim[2]` to `r new.full.modeled.dim[2]` columns.
```{r Correlation threshold definition}
cor.threshold <- .7
```
We still have to many columns to plot a correlation matrix. Therefore, we display only correlations that are above the value `r cor.threshold`.
```{r High Correlations function}
train.numeric <- train %>% dplyr::select_if(is.numeric)
cor.train <- cor(train.numeric)
cor.df <- as.data.frame(cor.train)

get.high.correlations <- function(cor.matrix, cor.threshold = .7, column.filter = NULL, max.elements = NULL){
  res.df <- which(
    cor.train - lower.tri(
      matrix(
        1, 
        nrow = nrow(cor.train),
        ncol = nrow(cor.train)
      ), 
      diag = TRUE
    ) > cor.threshold,
    arr.ind = TRUE
  ) %>%
    as.data.frame() %>% 
    `colnames<-`(c("c1", "c2")) %>% 
    mutate(
      Column1 = colnames(cor.train)[c1],
      Column2 = colnames(cor.train)[c2]
    ) %>% 
    mutate(
      Correlation = apply(., 1, function(row){
        cor.matrix[
          as.integer(row[which(names(row) == "c1")]), 
          as.integer(row[which(names(row) == "c2")])
        ]
      })
    ) %>% 
    select(Column1, Column2, Correlation) %>% 
    arrange(desc(Correlation))
  
  if(!is.null(column.filter)) res.df <- res.df %>% 
      filter(Column1 == column.filter | Column2 == column.filter)
  if(!is.null(max.elements)) res.df <- res.df %>% top_n(5, Correlation)
  res.df
}
```

```{r Printing tables side by side, results='asis', echo=FALSE}
    # Setting `results = 'asis'` allows for using Latex within the code chunk
    cat('\\begin{figure}')
    cat('\\begin{center}')
    # `{c c}` Creates a two column table
    # Use `{c | c}` if you'd like a line between the tables
    cat('\\begin{tabular}{ c c }')
    print(knitr::kable(
      get.high.correlations(cor.train, cor.threshold), 
      format = "latex"
    ))
    # Separate the two columns with an `&`
    cat('&')
    print(knitr::kable(
      get.high.correlations(cor.train, .01, "SalePrice", 5), 
      format = "latex"
    ))
    cat('\\end{tabular}')
    cat("\\caption{Most correlated columns (all | SalePrice)}")
    cat('\\end{center}')
    cat('\\end{figure}')
```
_AvgNeighbourSalePrice_ is a new feature obtained by grouping data by _SalePrice_ and calculating the mean. 
It is very interesting to see how _GarageArea_ and _GarageCars_ are important to Americans when determing house prices. The _OverallQual_ is also very considered, together with the _GrLivArea_. 

We still have a high number of features for the number of observations(`r dim(train.modeled)`). We will perform some variable selection to reduce the number of predictors.

# Modeling and Diagnostics

We start by fitting a linear model and try to validate the hypothesis inherent to the linear model.

```{r Full LM, warning=F}
res.lm.intercept <- lm(SalePrice~1, data = train)
res.full.lm <- lm(SalePrice~., data = train)
par(mfrow = c(2, 2))
plot(res.full.lm, which = c(1, 2, 3))
acf(residuals(res.full.lm), main = "")
```

P1) In the Residual vs Fitted plot help us the residual line is almost horizontal at zero. Thus, we can deduce that centered errors postulate is met.

P2) The homoscedasticity postulate is not verified as the standardized residuals line is not horizontal with equally spread points. It looks more like a parable. Also, the p-value of the _Breusch-Pagan_ test is `r ols_test_breusch_pagan(res.full.lm)$p`, rejecting the homoskedasticity hypothesis.

P3) On the autocorrelation plot, only one value is over the threshold but it is not high enough to question the uncorrelation of the errors. The _DurbinWatsonTest_ returns a p-value of `r durbinWatsonTest(res.full.lm)$p` and confirms that the errors are uncorrelated.

P4) The normality hypothesis is not verified when we look at the Q-Q plot, especially for the tails of the distribution. The result is confirmed by the _ShapiroTest_, which return a p-value of `r shapiro.test(residuals(res.full.lm))$p`.


In order to try to fix, or at least minimize, the heteroskedasticity of the data, we could try to take the log of the response variable.

```{r Logarithmic fit}
trainlog  <- train.modeled %>%
  as.data.frame() %>% 
  cbind(data.frame(SalePrice = train.y)) %>% 
  mutate(logSalePrice=log(SalePrice)) %>% 
  select(-SalePrice)

res.full.log.lm <- lm(logSalePrice~., data = trainlog)
```

```{r, warning = F}
pred <- predict(
  res.full.log.lm,
  newdata = test.modeled %>%
    as.data.frame()
)
rmses$log.lm <- sqrt(mean((exp(pred) - test.y)^2))
```

The log-transform of the response improves the diagnostic plots:

```{r, warning = F}
par(mfrow = c(2, 2))
plot(res.full.log.lm, which = c(1, 2, 3))
acf(residuals(res.full.log.lm), main = "")
```

The third plot shows now some improvement regarding homoscedasticity and we could work with this model instead. However, it is important to know th p-value of the _Breusch-Pagan_ test on this model is `r ols_test_breusch_pagan(res.full.log.lm)$p`. It is a bit higher but we still reject the homoskedasticity hipothesis.

The issues raised by heteroskedasticity are the following:

- The OLS estimators and regression predictions based on them remains unbiased and consistent.
- The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too.  
- Because of the inconsistency of the covariance matrix of the estimated regression coefficients, the tests of hypotheses, (t-test, F-test) are no longer valid.

### Outlier analysis.

```{r Cooks distance, warning=F}
plot(res.full.lm, which = 5)
```
We observe points with Cook distance greater than 1 so we can, at first sight, infer that there are no outliers. However, a more detailed outlier analysis is still necessary.

```{r InfluenceIndexPlot}
car::influenceIndexPlot(res.full.lm, vars=c("Studentized", "hat"))
```
The first plot makes it possible to identify outliers, many observations seem doubtful. The second plot shows the presence of many leverage points (with hate values above 0.5).
```{r, OutlierTest}
res.outlier.test <- car::outlierTest(res.full.lm, n.max = 20)
outliers.index <- paste(names(res.outlier.test$p), collapse = ", ")
```
We consider necessary to perform a Bonferroni _OutlierTest_ to identify outliers. The following points are labeled as such by the test: `r outliers.index`.

## Model Tuning

### Variable Selection

We use a stepwise method with the BIC criterion to determine the best model to use.

```{r Variable selection}
res.backward <- stepAIC(
  res.full.log.lm,
  SalePrice~1,
  trace = F,
  k = log(nrow(train)),
  direction=c('backward')
)
```

```{r}
summary(res.backward)
```


```{r, warning = F}
bic.kept.columns <- rownames(summary(res.backward)$coefficients)
bic.lm.res <- lm(
  SalePrice~.,
  data = train.modeled %>%
    as.data.frame() %>% 
    select(bic.kept.columns) %>%
    cbind(data.frame(SalePrice = train.y))
)
pred <- predict(
  bic.lm.res, 
  newdata = test.modeled[, rownames(summary(res.backward)$coefficients)] %>% as.data.frame()
)
rmses$bic.model <- sqrt(mean((pred - test.y)^2))
```

```{r}
as.data.frame(summary(res.backward)$coefficients) %>%
  rownames_to_column("colname") %>%
  rename("prob" = "Pr(>|t|)") %>%
  arrange(prob) %>%
  slice(1:15) %>%
  column_to_rownames("colname") %>% 
  select(prob)
```


### Hyper-parameter tuning

Uncomment to run, takes around 30 seconds to run.

```{r HyperParameter Tuning, warning = F}
res.caret <- caret::train(
  x = train.modeled,
  y = train.y,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = seq(0, 0.03, 0.002),
    lambda = seq(0, 1e5, 1000)
  ),
  metric = "RMSE"
)
```

```{r, Plot of res}
# plot(res.caret)
```

```{r, Optimal Parameters}
res.caret$bestTune

# test.data <- test.modeled %>% 
#   cbind(test.y) %>% 
#   `colnames<-`(c(colnames(test.modeled), "SalePrice")) %>% 
#   colnames()

optimal.model <- glmnet::glmnet(
  x = train.modeled, 
  y = train.y, 
  alpha = res.caret$bestTune$alpha, 
  lambda = res.caret$bestTune$lambda
)
res.pred.glmnet <- predict(optimal.model, newx = test.modeled) - test.y
sqrt(mean(res.pred.glmnet^2))
```


```{r, elasticNetFit}
predict.elsaticnet <- function(train.x, train.y, test.x, test.y, alpha = 0.01, lambda = c(1e5, .5e5, 32200, .25e5, 18000, 1e4, .5e4, 1e3, 1e2, 1e1, 1, 1e-1, 1e-2)){
  
  # Fit
  res.glmnet <- glmnet::glmnet(
    x = train.x,
    y = train.y,
    alpha = 0,
    lambda = lambda
  )
  
  # Prediction
  res.pred <- predict(res.glmnet, newx = test.x)
  colnames(res.pred) <- lambda
  # MSE
  mses <- apply(res.pred, 2, function(col){
    sum((col - test.y)^2)
  })
  names(mses) <- as.character(lambda)
  
  # Return object
  list(
    model = res.glmnet,
    pred = res.pred,
    mse = mses,
    lambda = lambda
  )
}

pred.res <- predict.elsaticnet(
  train.x = train.modeled, 
  train.y = train.y, 
  test.x = test.modeled, 
  test.y = test.y
)
# plot(sqrt(pred.res$mse) ~ pred.res$lambda, log = "x", type = "o", main = "RMSE=f(lambda)", xlab="lambda", ylab="RMSE")
```

### Anova

```{r}
anova.full <- anova(res.full.lm)
as.data.frame(anova.full) %>% 
  rownames_to_column("rown") %>% 
  rename("prob" = "Pr(>F)") %>% 
  filter(prob < 0.001) %>% 
  select(prob, rown) %>% 
  arrange(prob) %>% 
  slice(1:5) %>% 
  column_to_rownames("rown") 
```

The output of the Anova makes a lot of sense. Indeed, the most signifcant variables are the ones that comes to one's mind when estiamting the price of a house : Type of building sold (MSSubClass), Neighborhood where the place is located, Overall quality of the good, the area of the place and the linear feet of street connected to property.

# Final Models

```{r}
library(randomForest)
randomForest::randomForest(x = train.modeled, y = train.y, xtest = test.modeled, ytest = test.y)
```

```{r}
gbm.res <- gbm::gbm(
  SalePrice ~ ., 
  data = train.x %>% cbind(data.frame(SalePrice = train.y)),
    # train.modeled %>%
    # as.data.frame() %>%
    # cbind(data.frame(SalePrice = train.y)),
  distribution = "gaussian",
  n.trees = 500,
  interaction.depth	= 2,
  n.minobsinnode = 10,
  cv.folds = 5
)
pred <- predict(gbm.res, newdata = test.x, n.trees = 500) - test.y
sqrt(mean(pred^2))
```

```{r}
xgboost.res <- xgboost::xgboost(
  data = train.modeled,
  label = train.y,
  nrounds = 100,
  verbose = 0
)
pred <- predict(xgboost.res, newdata = test.modeled) - test.y
sqrt(mean(pred^2))
```


```{r}
data.frame(Model = names(rmses), RMSE = unlist(rmses))
```


# Discussion

If we had had more rows, we could have kept more columns and have a better model. Is it relevant to say that?


